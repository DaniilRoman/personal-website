[
    {
        "title": "Make data migration easy with Debezium and Apache Kafka",
        "pubDate": "2025-01-28 09:46:44",
        "link": "https://medium.com/adevinta-tech-blog/make-data-migration-easy-with-debezium-and-apache-kafka-4c9e2b9b6601?source=rss-ea55ac2dd44d------2",
        "guid": "https://medium.com/p/4c9e2b9b6601",
        "author": "Daniil Roman",
        "thumbnail": "https://cdn-images-1.medium.com/max/1024/0*Dog_js7-NaQ3eGaW",
        "description": "How Team Search has built an ETL pipeline with a throughput of 3000 events per second",
        "content": "\n<p>How Team Search has built an ETL pipeline with a throughput of 3000 events per\u00a0second</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Dog_js7-NaQ3eGaW\"></figure><p>At <a href=\"https://adevinta.com/brand/kleinanzeigen/\"><strong>Kleinanzeigen</strong></a>, Germany\u2019s leading second-hand marketplace, we recently tackled a fascinating yet challenging task: migrating data from one system to\u00a0another.</p>\n<p>In this article, I\u2019ll share the story of our journey\u200a\u2014\u200awhy it was challenging for our team I\u2019ll also describe the technical path we followed and the lessons we learned along the\u00a0way.</p>\n<p>Let\u2019s dive in\u00a0\ud83d\ude80</p>\n<h4>Why might this guide be interesting for\u00a0you?</h4>\n<ul>\n<li>You\u2019re facing a similar\u00a0task</li>\n<li>Old approaches don\u2019t work\u00a0anymore</li>\n<li>You already use\u00a0Kafka</li>\n<li>The complexity of your systems is\u00a0growing</li>\n</ul>\n<h3>What did our task look\u00a0like?</h3>\n<p>Why might this process be interesting to you? Data migration is an increasingly relevant topic as systems grow more complex. With many companies transitioning from monolithic architectures to microservices, traditional approaches often fall short. When migrating large volumes of data in real-time, you can\u2019t afford data loss or downtime.</p>\n<p>Let\u2019s take a closer look at the task we\u00a0faced.</p>\n<blockquote>\n<em>Disclaimer</em>: The data schema fields shown here are artificial. While conceptually similar to the actual schema we worked with, it is not identical.</blockquote>\n<p>Our main challenge was to merge three parts of data into a single \u201cGod object,\u201d which contains all the necessary information for each\u00a0user_id.</p>\n<ul>\n<li>Some data resided in the same MySQL database but across different tables.</li>\n<li>The remaining data was hosted in an external service, which we accessed via\u00a0HTTP.</li>\n</ul>\n<figure><img alt=\"On the left side there are three parts of the data in JSON format and on the right there\u2019s a final object in JSON format with all necessary fields: user_id, new_user_id, email and dark_mode\" src=\"https://cdn-images-1.medium.com/max/1024/0*c-KzOKNH7mJL7PPm\"><figcaption>De-normalisation overview for our data on how 3 source JSON objects should look at the end of the ETL\u00a0pipeline</figcaption></figure><p>Our team primarily works on classic backend services, not data/ETL pipelines. While we were familiar with <strong>Apache Kafka</strong> as a message broker, tools like <strong>Debezium</strong> and <strong>Kafka Streams</strong> weren\u2019t part of our existing\u00a0toolkit.</p>\n<h3><strong>How did we break this down into software components?</strong></h3>\n<p>Let\u2019s break the task into actionable steps to clarify our approach.</p>\n<figure><img alt=\"On the left side there are three parts of the data in JSON format and on the right there\u2019s a final object in JSON format with all necessary fields: user_id, new_user_id, email and dark_mode\" src=\"https://cdn-images-1.medium.com/proxy/0*3y1F-UMKTEep4U8p\"><figcaption>Three steps we need to implement</figcaption></figure><p>As shown in the\u00a0diagram:</p>\n<ol>\n<li>\n<strong>Streaming Data from MySQL to Kafka</strong>:<br>The first step was streaming data from the MySQL database to a Kafka topic. This ensured we could track all real-time modifications in the database.</li>\n<li>\n<strong>Aggregating Data Across MySQL Tables</strong>:<br>We needed to combine data stored in different MySQL tables. Conceptually, this was akin to running a join query\u00a0like:</li>\n</ol>\n<pre>SELECT u.user_id, u.email, p.dark_mode<br>FROM users u<br>JOIN preferences p ON u.user_id = p.user_id;</pre>\n<p>The challenge was ensuring this aggregation occurred in real-time.</p>\n<p>3.<strong> Fetching External Data</strong>:<br>The final step involved fetching additional data from an external service to complete the \u201cGod object.\u201d Since we didn\u2019t control this service, we needed a caching mechanism for the aggregated MySQL data. This way, if the database changes were reprocessed, we could avoid redundant table aggregations.</p>\n<p>On a data level, the transformation looked like\u00a0this:</p>\n<figure><img alt=\"The diagram with three steps of the data transformation. On the diagram you\u2019ll see the data looks on each step on this de-normalisation process of User data. The first step of de-normalisation is to merge email and dark_mode and then on the last part we merge a new_user_id as well.\" src=\"https://cdn-images-1.medium.com/max/1024/0*6rggL8ZuFuTNV4Ez\"><figcaption>The data transformation we need to implement on each\u00a0step</figcaption></figure><ul>\n<li>\n<strong>MySQL</strong> provided the email and dark_mode fields.</li>\n<li>An external service supplied the ID mappings to complete the denormalised \u201cGod\u00a0object.\u201d</li>\n</ul>\n<h3>Why Apache\u00a0Kafka?</h3>\n<figure><img alt=\"The Debezium, Kafka Streams and Kafka logos\" src=\"https://cdn-images-1.medium.com/max/1024/0*y5fbspnKfVNM6bEM\"><figcaption>Elements of the Apache Kafka ecosystem</figcaption></figure><p>You might wonder, \u201cWhy are we talking about Apache Kafka for this use\u00a0case?\u201d</p>\n<p>While we already use Apache Kafka extensively as a message broker, its ecosystem offers much more than basic messaging. Here\u2019s what makes Kafka such a powerful\u00a0tool:</p>\n<ul>\n<li>\n<strong>Debezium</strong>: A Change Data Capture (CDC) solution to stream database changes into Kafka as\u00a0events.</li>\n<li>\n<strong>MirrorMaker</strong>: Helps mirror topics from one Kafka cluster to\u00a0another.</li>\n<li>\n<strong>Kafka Streams</strong>: Enables in-flight data transformations and aggregations.</li>\n<li>\n<strong>Compacted Topics</strong>: Built-in configuration that allows Kafka topics to behave like a cache, storing the latest snapshots of events for each unique\u00a0key.</li>\n</ul>\n<p>With these tools, Kafka becomes an end-to-end platform for streaming, processing, and storing event-driven data.</p>\n<h3>Steps we\u00a0took</h3>\n<p>Let\u2019s walk through the steps we followed to achieve our\u00a0goal.</p>\n<p>We realised that in addition to the usual Kafka producer/consumer functionality, we\u00a0needed:</p>\n<ol>\n<li>A <strong>compacted topic</strong> to aggregate changes from the database.</li>\n<li>\n<strong>Debezium</strong> as a CDC solution to stream real-time database changes into\u00a0Kafka.</li>\n</ol>\n<p>However, we decided against using Kafka Streams to aggregate data across MySQL tables and Kafka topics, as it wasn\u2019t necessary for our specific use\u00a0case.</p>\n<h4>1. Streaming MySQL modifications with\u00a0Debezium</h4>\n<p>The first challenge was streaming real-time changes from MySQL. To choose the right tool, we had to consider two key nuances of our\u00a0task:</p>\n<ol>\n<li>The data arrives continuously in real-time, meaning we couldn\u2019t take a one-time snapshot.</li>\n<li>We needed to ensure every record in MySQL was processed at least once, including historical data.</li>\n</ol>\n<p><strong>Debezium</strong> proved to be the perfect fit for our requirements.</p>\n<p>According to the<a href=\"https://debezium.io/\"> official\u00a0website</a>:</p>\n<blockquote><em>Debezium is an open-source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go\u00a0wrong.</em></blockquote>\n<p>If you already have existing Kafka Connect and Debezium infrastructure, setting up a new Debezium connector is straightforward.</p>\n<p>That said, there is one drawback: Troubleshooting can be challenging because issues often arise at the infrastructure level rather than in the application code.</p>\n<p>From an architectural perspective, the setup is simple\u200a\u2014\u200awe reused the existing Debezium and Kafka Connect clusters.</p>\n<figure><img alt=\"The first part of our pipeline where we stream data from MySQL database to Kafka by using Debezium\" src=\"https://cdn-images-1.medium.com/max/1024/0*LQqbrGDqsLFJUj6g\"><figcaption>The first part of our pipeline where we stream data from MySQL database to\u00a0Kafka</figcaption></figure><p>No code required: Setting up the Debezium connector</p>\n<p>Creating a new connector didn\u2019t require writing any application code. Instead, we defined a configuration file, which was similar to this example from the official documentation:</p>\n<pre>{<br>  \"name\": \"mysql-connector\",<br>  \"config\": {<br>    \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",<br>    \"tasks.max\": \"1\",<br>    \"database.hostname\": \"localhost\",<br>    \"database.port\": \"3306\",<br>    \"database.user\": \"debezium\",<br>    \"database.password\": \"dbz\",<br>    \"database.server.id\": \"184054\",<br>    \"database.server.name\": \"fullfillment\",<br>    \"database.include.list\": \"inventory\",<br>    \"table.include.list\": \"inventory.orders\",<br>    \"snapshot.mode\": \"initial\"<br>  }<br>}</pre>\n<p>This configuration allows you\u00a0to:</p>\n<ul>\n<li>\n<strong>Filter data</strong>: Specify the databases and tables to\u00a0monitor.</li>\n<li>\n<strong>Modify data</strong>: Apply simple transformations at the configuration level.</li>\n<li>\n<strong>Control snapshots</strong>: Use the <strong>snapshot.mode</strong> option to define criteria for capturing the initial state of the database.</li>\n</ul>\n<p>By leveraging Debezium and Kafka Connect, we were able to stream real-time database changes efficiently without writing a single line of application code.</p>\n<h3>2. Aggregating Data</h3>\n<p>The next step involved aggregating data for the same <strong>user_id</strong> stored across different MySQL tables. Here\u2019s where things got interesting: one table had relatively infrequent updates to <strong>email</strong> data, while another had frequent updates to <strong>dark_mode</strong> information.</p>\n<p>We could have used <strong>Kafka Streams</strong> for this purpose, leveraging RocksDB to store intermediate states. However, we chose a more pragmatic approach to avoid adding unnecessary architectural complexity.</p>\n<figure><img alt=\"The second part of our pipeline where we aggregate event data from twoKafka topics. We use MySQL for this instead of Kafka Streams\" src=\"https://cdn-images-1.medium.com/max/1024/0*9StGmDIquFpK1z64\"><figcaption>The second part of our pipeline where we aggregate event data from two Kafka\u00a0topics</figcaption></figure><h4>Our approach</h4>\n<p>The idea was straightforward:</p>\n<ol>\n<li>Listen for every data modification event.</li>\n<li>Extract the aggregated data directly from MySQL using a single SQL\u00a0query.</li>\n<li>Send the aggregated result to an output Kafka\u00a0topic.</li>\n</ol>\n<p>A simple SQL query like the one below handled the aggregation:</p>\n<pre>SELECT u.user_id, u.email, p.dark_mode  <br>FROM users u  <br>JOIN preferences p ON u.user_id = p.user_id  <br>WHERE u.user_id = 1;</pre>\n<p><strong>Benefits<br></strong>The simplicity of this setup was its biggest advantage. By listening for Kafka events, fetching the data directly with SQL, and aggregating it, we avoided introducing additional components like Kafka\u00a0Streams.</p>\n<p><strong>Drawbacks<br></strong>While effective, this approach could become a bottleneck under high load, as processing events would depend on fetching data from MySQL in real-time.</p>\n<p>For our use case, this wasn\u2019t a concern\u00a0because:</p>\n<ol>\n<li>Our load wasn\u2019t high enough to cause\u00a0delays.</li>\n<li>Any potential bottlenecks were mitigated downstream by aggregating data in a Kafka topic, especially during full re-consumption of MySQL\u00a0changes.</li>\n</ol>\n<h4><strong>3. Using a compacted Kafka topic for quick re-consumption</strong></h4>\n<p>By this stage, our pipeline was functional. However, there was one additional challenge: re-consuming all MySQL data whenever changes occurred in third-party services that we didn\u2019t\u00a0control.</p>\n<p>To address this, we used a <strong>compacted Kafka\u00a0topic</strong>.</p>\n<p><strong>How Compacted Kafka Topics Work<br></strong>In a compacted topic, Kafka retains only the latest event for each unique key. Here\u2019s a simple illustration:</p>\n<p><strong>First Event for\u00a0id=1:</strong></p>\n<pre>{<br>  \"id\": 1,<br>  \"data\": \"first data change\" <br>}</pre>\n<p><strong>Second Event for\u00a0id=1:</strong></p>\n<pre>{<br>  \"id\": 1,<br>  \"data\": \"second data change\" <br>}</pre>\n<p>The second event overwrites the first for the same key (<strong>id=1</strong>). This ensures the most recent event is always available and stored indefinitely.</p>\n<p><strong>Benefits<br></strong>Using a compacted topic allowed us to efficiently re-consume MySQL changes whenever needed. We\u00a0could:</p>\n<ul>\n<li>Modify the offset of an existing consumer\u00a0group.</li>\n<li>Create a new consumer group to fetch all the latest snapshots stored in the compacted topic.</li>\n</ul>\n<p>This setup ensured that all aggregated MySQL data was readily available, even when a full re-consumption was required.</p>\n<figure><img alt=\"The third part of our pipeline where we call an external service to fetch the latest part of the data.\" src=\"https://cdn-images-1.medium.com/max/1024/0*qaYoV6yFkx9JJouj\"><figcaption>The third part of our pipeline where we call an external service to fetch the latest part of the\u00a0data</figcaption></figure><h3>Results</h3>\n<p>Our final migration pipeline ended up looking like the diagram\u00a0below:</p>\n<figure><img alt=\"The final architecture split into three sections. The first streaming data from a database. The second join events from two Kafka topics into one. The third cache all denormalised user data.\" src=\"https://cdn-images-1.medium.com/max/1024/0*pfERX4E84mHZ-Bm1\"><figcaption>The final architecture</figcaption></figure><p>We successfully achieved our goals without introducing unnecessary technologies while maintaining a high throughput for the pipeline. Here\u2019s an example of the throughput we achieved:</p>\n<figure><img alt=\"The throughput of number of messages sent per second in each section of the architecture. In section one up to 50k per second. In section two up to 10k per second. In section three up to 3k per second.\" src=\"https://cdn-images-1.medium.com/max/1024/0*DiIiMvyj9LHQbDJ6\"><figcaption>The throughput numbers we\u2019ve\u00a0got</figcaption></figure><p>Below is a snapshot of the actual data flowing through part of our pipeline, as visualised in our Grafana dashboard:</p>\n<figure><img alt=\"The numbers from our Grafana dashboard. The same as from the previous image. Up to 50k messages per second come from Debezium, up to 10k messages per second come from our aggregator and up to 30k messages per second come when we call an external service at the end of our pipeline.\" src=\"https://cdn-images-1.medium.com/max/1024/0*ATA71Hj0ljK9vKhn\"><figcaption>The real numbers of the throughput of our pipeline from Grafana dashboard</figcaption></figure><p>The numbers shown reflect a re-consumption scenario where we processed all available data, pushing our pipeline\u2019s throughput to its\u00a0limits.</p>\n<h3><strong>Key Learnings</strong></h3>\n<p>Through this project, we gained several insights:</p>\n<ul>\n<li>\n<strong>Debezium is powerful but not without challenges.<br></strong>While Debezium is an excellent tool for change data capture (CDC), troubleshooting issues can sometimes be complex, as they often occur at the infrastructure level rather than in the application code.</li>\n<li>\n<strong>Compacted Kafka topics simplify the architecture.<br></strong>Introducing a compacted Kafka topic provided a streamlined way to manage potential bottlenecks during re-processing, ensuring that the most recent state was always readily accessible.</li>\n</ul>\n<h4><strong>And as the final\u00a0words:</strong></h4>\n<ul>\n<li>Write configuration instead of\u00a0code</li>\n<li>Know your tool. There are a lot around Apache\u00a0Kafka</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4c9e2b9b6601\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/adevinta-tech-blog/make-data-migration-easy-with-debezium-and-apache-kafka-4c9e2b9b6601\">Make data migration easy with Debezium and Apache Kafka</a> was originally published in <a href=\"https://medium.com/adevinta-tech-blog\">Adevinta Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
        "enclosure": {},
        "categories": [
            "debezium",
            "apache-kafka",
            "etl",
            "data-migration"
        ]
    },
    {
        "type_of": "article",
        "id": 1635330,
        "title": "GitHub Innovation Graph analysis",
        "description": "GitHub has recently unveiled its Innovation Graph, offering us a high level view of the global tech...",
        "readable_publish_date": "Oct 15 '23",
        "slug": "github-innovation-graph-analysis-27m3",
        "path": "/daniilroman/github-innovation-graph-analysis-27m3",
        "url": "https://dev.to/daniilroman/github-innovation-graph-analysis-27m3",
        "comments_count": 0,
        "public_reactions_count": 1,
        "collection_id": null,
        "published_timestamp": "2023-10-15T13:10:53Z",
        "language": "en",
        "subforem_id": null,
        "positive_reactions_count": 1,
        "cover_image": "https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F80poj8my1dmkyjjwcaj9.jpeg",
        "social_image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F80poj8my1dmkyjjwcaj9.jpeg",
        "canonical_url": "https://dev.to/daniilroman/github-innovation-graph-analysis-27m3",
        "created_at": "2023-10-15T13:10:54Z",
        "edited_at": null,
        "crossposted_at": null,
        "published_at": "2023-10-15T13:10:53Z",
        "last_comment_at": "2023-10-15T13:10:53Z",
        "reading_time_minutes": 10,
        "tag_list": [
            "github",
            "innovationgraph",
            "analytics"
        ],
        "tags": "github, innovationgraph, analytics",
        "user": {
            "name": "Daniil Roman",
            "username": "daniilroman",
            "twitter_username": null,
            "github_username": "DaniilRoman",
            "user_id": 854738,
            "website_url": null,
            "profile_image": "https://media2.dev.to/dynamic/image/width=640,height=640,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg",
            "profile_image_90": "https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg"
        }
    },
    {
        "type_of": "article",
        "id": 1128517,
        "title": "Re2j instead of default regEx in Java: when and how to use it",
        "description": "Hi everyone! I\u2019m Daniil and I\u2019m a Java developer at Just AI. In this article, I\u2019m going to tell you...",
        "readable_publish_date": "Jun 30 '22",
        "slug": "re2j-instead-of-default-regex-in-java-when-and-how-to-use-it-5bgn",
        "path": "/daniilroman/re2j-instead-of-default-regex-in-java-when-and-how-to-use-it-5bgn",
        "url": "https://dev.to/daniilroman/re2j-instead-of-default-regex-in-java-when-and-how-to-use-it-5bgn",
        "comments_count": 0,
        "public_reactions_count": 8,
        "collection_id": null,
        "published_timestamp": "2022-06-30T12:21:43Z",
        "language": null,
        "subforem_id": null,
        "positive_reactions_count": 8,
        "cover_image": "https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F47kggas795bhvlqlwdhf.jpg",
        "social_image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F47kggas795bhvlqlwdhf.jpg",
        "canonical_url": "https://dev.to/daniilroman/re2j-instead-of-default-regex-in-java-when-and-how-to-use-it-5bgn",
        "created_at": "2022-06-30T12:21:43Z",
        "edited_at": "2022-06-30T15:43:44Z",
        "crossposted_at": null,
        "published_at": "2022-06-30T12:21:43Z",
        "last_comment_at": "2022-06-30T12:21:43Z",
        "reading_time_minutes": 6,
        "tag_list": [
            "java",
            "kotlin",
            "regex",
            "re2j"
        ],
        "tags": "java, kotlin, regex, re2j",
        "user": {
            "name": "Daniil Roman",
            "username": "daniilroman",
            "twitter_username": null,
            "github_username": "DaniilRoman",
            "user_id": 854738,
            "website_url": null,
            "profile_image": "https://media2.dev.to/dynamic/image/width=640,height=640,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg",
            "profile_image_90": "https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg"
        }
    },
    {
        "type_of": "article",
        "id": 1121041,
        "title": "Maven plugin to check dependency versions",
        "description": "Hi everyone! In this post I\u2019m gonna show you why you have to check your dependency versions in your...",
        "readable_publish_date": "Jun 22 '22",
        "slug": "maven-plugin-to-check-dependencies-versions-318m",
        "path": "/daniilroman/maven-plugin-to-check-dependencies-versions-318m",
        "url": "https://dev.to/daniilroman/maven-plugin-to-check-dependencies-versions-318m",
        "comments_count": 1,
        "public_reactions_count": 13,
        "collection_id": null,
        "published_timestamp": "2022-06-22T06:22:49Z",
        "language": null,
        "subforem_id": null,
        "positive_reactions_count": 13,
        "cover_image": "https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F72ebr606rvtzub81ozqw.jpg",
        "social_image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F72ebr606rvtzub81ozqw.jpg",
        "canonical_url": "https://dev.to/daniilroman/maven-plugin-to-check-dependencies-versions-318m",
        "created_at": "2022-06-22T06:22:49Z",
        "edited_at": "2022-07-01T08:46:03Z",
        "crossposted_at": null,
        "published_at": "2022-06-22T06:22:49Z",
        "last_comment_at": "2022-06-26T17:52:48Z",
        "reading_time_minutes": 3,
        "tag_list": [
            "java",
            "maven",
            "kotlin",
            "tutorial"
        ],
        "tags": "java, maven, kotlin, tutorial",
        "user": {
            "name": "Daniil Roman",
            "username": "daniilroman",
            "twitter_username": null,
            "github_username": "DaniilRoman",
            "user_id": 854738,
            "website_url": null,
            "profile_image": "https://media2.dev.to/dynamic/image/width=640,height=640,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg",
            "profile_image_90": "https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg"
        }
    },
    {
        "type_of": "article",
        "id": 1071574,
        "title": "Designing an application with Redis as a data store. What? Why?",
        "description": "1) Introduction   Hello everyone! Many people know what Redis is, and if you don\u2019t know, the...",
        "readable_publish_date": "Apr 30 '22",
        "slug": "designing-an-application-with-redis-as-a-data-store-what-why-57e3",
        "path": "/daniilroman/designing-an-application-with-redis-as-a-data-store-what-why-57e3",
        "url": "https://dev.to/daniilroman/designing-an-application-with-redis-as-a-data-store-what-why-57e3",
        "comments_count": 0,
        "public_reactions_count": 7,
        "collection_id": null,
        "published_timestamp": "2022-04-30T07:57:21Z",
        "language": null,
        "subforem_id": null,
        "positive_reactions_count": 7,
        "cover_image": "https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fh4gn35c6zrrh0ktk9g7y.png",
        "social_image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fh4gn35c6zrrh0ktk9g7y.png",
        "canonical_url": "https://medium.com/@daniil_roman/designing-an-application-with-redis-as-a-data-store-what-why-d02e685ee2b8",
        "created_at": "2022-04-30T07:57:21Z",
        "edited_at": "2022-04-30T08:00:38Z",
        "crossposted_at": null,
        "published_at": "2022-04-30T07:57:21Z",
        "last_comment_at": "2022-04-30T07:57:21Z",
        "reading_time_minutes": 7,
        "tag_list": [
            "redis",
            "kotlin",
            "java",
            "architecture"
        ],
        "tags": "redis, kotlin, java, architecture",
        "user": {
            "name": "Daniil Roman",
            "username": "daniilroman",
            "twitter_username": null,
            "github_username": "DaniilRoman",
            "user_id": 854738,
            "website_url": null,
            "profile_image": "https://media2.dev.to/dynamic/image/width=640,height=640,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg",
            "profile_image_90": "https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg"
        }
    },
    {
        "type_of": "article",
        "id": 1069702,
        "title": "How we built Elasticsearch index",
        "description": "What we wanted to do   Hey everyone. I\u2019m Daniil and I work at Just AI where we are building...",
        "readable_publish_date": "Apr 28 '22",
        "slug": "how-we-built-elasticsearch-index-8hh",
        "path": "/daniilroman/how-we-built-elasticsearch-index-8hh",
        "url": "https://dev.to/daniilroman/how-we-built-elasticsearch-index-8hh",
        "comments_count": 0,
        "public_reactions_count": 11,
        "collection_id": null,
        "published_timestamp": "2022-04-28T12:31:42Z",
        "language": null,
        "subforem_id": null,
        "positive_reactions_count": 11,
        "cover_image": "https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjy3y8d5oi53scwfyqjmg.png",
        "social_image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjy3y8d5oi53scwfyqjmg.png",
        "canonical_url": "https://dev.to/daniilroman/how-we-built-elasticsearch-index-8hh",
        "created_at": "2022-04-28T12:31:42Z",
        "edited_at": "2022-04-30T13:53:02Z",
        "crossposted_at": null,
        "published_at": "2022-04-28T12:31:42Z",
        "last_comment_at": "2022-04-28T12:31:42Z",
        "reading_time_minutes": 7,
        "tag_list": [
            "database",
            "architecture",
            "elasticsearch"
        ],
        "tags": "database, architecture, elasticsearch",
        "user": {
            "name": "Daniil Roman",
            "username": "daniilroman",
            "twitter_username": null,
            "github_username": "DaniilRoman",
            "user_id": 854738,
            "website_url": null,
            "profile_image": "https://media2.dev.to/dynamic/image/width=640,height=640,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg",
            "profile_image_90": "https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F854738%2Fc41bf28b-90ae-4936-8dbe-088341ec80df.jpeg"
        }
    }
]